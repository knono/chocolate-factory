"""
Full Pipeline Tests - Sprint 12 Fase 11

Tests end-to-end del flujo completo de datos desde ingesta hasta visualizaci√≥n.
Validan que todos los componentes del sistema funcionan correctamente integrados.

Flujos testeados:
1. REE ingestion ‚Üí InfluxDB ‚Üí Dashboard
2. Weather ingestion ‚Üí Feature engineering ‚Üí ML prediction
3. Prophet forecasting ‚Üí Price predictions ‚Üí Dashboard
4. Hourly optimization ‚Üí Timeline generation
5. Backfill recovery ‚Üí Gap filling

Ejecuci√≥n:
    pytest tests/e2e/test_full_pipeline.py -v -m e2e

Autor: Sprint 12 Fase 11
Fecha: 2025-10-20
"""

import pytest
import httpx
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List


BASE_URL = "http://localhost:8000"
TIMEOUT = 15.0  # Pipelines pueden tardar m√°s


@pytest.fixture(scope="module")
def api_client():
    """Cliente HTTP para tests de pipeline"""
    with httpx.Client(base_url=BASE_URL, timeout=TIMEOUT) as client:
        yield client


# ============================================================================
# PIPELINE TESTS - Flujos end-to-end completos
# ============================================================================

@pytest.mark.e2e
@pytest.mark.pipeline
class TestREEIngestionPipeline:
    """Test 1/5: Pipeline completo REE ingestion ‚Üí Dashboard"""

    def test_ree_ingestion_to_dashboard_display(self, api_client: httpx.Client):
        """
        Flujo completo: Ingesta REE ‚Üí InfluxDB ‚Üí Dashboard

        Steps:
        1. Verificar que hay datos REE recientes en dashboard
        2. Validar estructura de datos de precios
        3. Confirmar que se actualizan peri√≥dicamente
        """

        # Step 1: Obtener datos actuales del dashboard
        response = api_client.get("/dashboard/complete")
        assert response.status_code == 200, f"Dashboard failed: {response.text}"

        data = response.json()
        assert "current_data" in data, "Missing current_data"

        current = data["current_data"]

        # Step 2: Validar datos de precio REE
        assert "current_price" in current, "Missing current_price"
        price = current["current_price"]

        assert price is not None, "Current price is None"
        assert isinstance(price, (int, float)), f"Price should be numeric, got {type(price)}"
        assert 0 < price < 1.0, f"Price {price} out of expected range (0-1 ‚Ç¨/kWh)"

        print(f"‚úÖ REE Pipeline: Data flowing")
        print(f"üí∞ Current price: {price:.4f} ‚Ç¨/kWh")

        # Step 3: Verificar datos hist√≥ricos disponibles
        if "historical_data" in data:
            historical = data["historical_data"]
            assert len(historical) > 0, "No historical data available"
            print(f"üìä Historical records: {len(historical)}")

        # Step 4: Validar que el precio tiene timestamp reciente
        if "timestamp" in current or "last_update" in current:
            timestamp_field = "timestamp" if "timestamp" in current else "last_update"
            timestamp = current[timestamp_field]
            print(f"üïê Last update: {timestamp}")


@pytest.mark.e2e
@pytest.mark.pipeline
class TestWeatherToMLPipeline:
    """Test 2/5: Weather ingestion ‚Üí Feature engineering ‚Üí ML prediction"""

    def test_weather_ingestion_to_ml_prediction(self, api_client: httpx.Client):
        """
        Flujo: Weather data ‚Üí Feature engineering ‚Üí ML prediction

        Steps:
        1. Verificar datos weather disponibles
        2. Confirmar feature engineering funcionando
        3. Validar que ML genera predicciones
        """

        # Step 1: Obtener datos de clima actuales
        response = api_client.get("/dashboard/complete")
        assert response.status_code == 200, f"Dashboard failed: {response.text}"

        data = response.json()
        current = data.get("current_data", {})

        # Step 2: Validar datos de clima
        assert "current_weather" in current, "Missing weather data"
        weather = current["current_weather"]

        assert "temperature" in weather, "Missing temperature"
        assert "humidity" in weather, "Missing humidity"

        temp = weather["temperature"]
        humidity = weather["humidity"]

        assert -20 < temp < 50, f"Temperature {temp}¬∞C out of range"
        assert 0 <= humidity <= 100, f"Humidity {humidity}% out of range"

        print(f"‚úÖ Weather Pipeline: Data available")
        print(f"üå°Ô∏è  Temperature: {temp}¬∞C")
        print(f"üíß Humidity: {humidity}%")

        # Step 3: Verificar que hay predicciones ML basadas en weather
        if "predictions" in data or "ml_insights" in data:
            predictions_key = "predictions" if "predictions" in data else "ml_insights"
            predictions = data[predictions_key]

            # Las predicciones usan datos de weather como features
            assert predictions is not None, "ML predictions missing"
            print(f"‚úÖ ML predictions generated from weather data")


@pytest.mark.e2e
@pytest.mark.pipeline
class TestProphetForecastingPipeline:
    """Test 3/5: Prophet training ‚Üí 168h forecast ‚Üí Dashboard"""

    def test_prophet_forecasting_pipeline(self, api_client: httpx.Client):
        """
        Flujo: Datos hist√≥ricos REE ‚Üí Prophet training ‚Üí 7-day forecast

        Steps:
        1. Solicitar forecast de 7 d√≠as (168h)
        2. Validar estructura de predicciones
        3. Verificar intervalos de confianza
        4. Confirmar que aparece en dashboard
        """

        # Step 1: Obtener forecast semanal de Prophet
        response = api_client.get("/predict/prices/weekly")
        assert response.status_code == 200, f"Prophet forecast failed: {response.text}"

        data = response.json()

        # Step 2: Validar estructura
        assert "predictions" in data, "Missing predictions"
        predictions = data["predictions"]

        assert len(predictions) >= 168, \
            f"Expected 168 predictions (7 days), got {len(predictions)}"

        # Step 3: Validar cada predicci√≥n tiene campos requeridos
        first_pred = predictions[0]
        required_fields = ["timestamp", "predicted_price", "confidence_lower", "confidence_upper"]

        for field in required_fields:
            assert field in first_pred, f"Missing field: {field}"

        # Validar que confidence intervals son coherentes
        assert first_pred["confidence_lower"] < first_pred["predicted_price"], \
            "Lower bound should be less than predicted price"
        assert first_pred["predicted_price"] < first_pred["confidence_upper"], \
            "Predicted price should be less than upper bound"

        print(f"‚úÖ Prophet Pipeline: Complete")
        print(f"üìà Generated {len(predictions)} hourly predictions")
        print(f"üí∞ First prediction: {first_pred['predicted_price']:.4f} ‚Ç¨/kWh")
        print(f"üìä Confidence interval: [{first_pred['confidence_lower']:.4f}, "
              f"{first_pred['confidence_upper']:.4f}]")

        # Step 4: Verificar m√©tricas del modelo si disponibles
        if "model_metrics" in data:
            metrics = data["model_metrics"]
            print(f"üìä Model MAE: {metrics.get('mae', 'N/A')}")
            print(f"üìä Model R¬≤: {metrics.get('r2', 'N/A')}")


@pytest.mark.e2e
@pytest.mark.pipeline
class TestHourlyOptimizationPipeline:
    """Test 4/5: Price forecast ‚Üí Production optimization ‚Üí Timeline"""

    def test_hourly_optimization_flow(self, api_client: httpx.Client):
        """
        Flujo: Prophet prices ‚Üí Hourly optimizer ‚Üí 24h timeline

        Steps:
        1. Solicitar optimizaci√≥n diaria
        2. Validar timeline horaria (24 elementos)
        3. Verificar per√≠odos tarifarios (P1/P2/P3)
        4. Confirmar que usa predicciones Prophet
        """

        # Step 1: Solicitar optimizaci√≥n para ma√±ana
        tomorrow = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")

        payload = {
            "target_date": tomorrow,
            "target_kg": 200
        }

        response = api_client.post("/optimize/production/daily", json=payload)
        assert response.status_code == 200, f"Optimization failed: {response.text}"

        data = response.json()

        # Step 2: Validar timeline horaria
        assert "hourly_timeline" in data, "Missing hourly_timeline"
        timeline = data["hourly_timeline"]

        assert len(timeline) == 24, f"Expected 24 hours, got {len(timeline)}"

        # Step 3: Validar estructura de cada hora
        first_hour = timeline[0]
        required_fields = [
            "hour", "time", "price_eur_kwh", "tariff_period",
            "tariff_color", "temperature", "humidity"
        ]

        for field in required_fields:
            assert field in first_hour, f"Missing field in timeline: {field}"

        # Step 4: Validar per√≠odos tarifarios
        tariff_periods = set(hour["tariff_period"] for hour in timeline)
        valid_periods = {"P1", "P2", "P3"}

        assert tariff_periods.issubset(valid_periods), \
            f"Invalid tariff periods: {tariff_periods - valid_periods}"

        print(f"‚úÖ Optimization Pipeline: Complete")
        print(f"üìÖ Optimized for: {tomorrow}")
        print(f"‚è∞ Timeline hours: {len(timeline)}")
        print(f"üéØ Target production: {payload['target_kg']} kg")

        # Step 5: Validar que hay batches recomendados
        if "batches" in data:
            batches = data["batches"]
            print(f"üì¶ Recommended batches: {len(batches)}")


@pytest.mark.e2e
@pytest.mark.pipeline
@pytest.mark.slow
class TestBackfillRecoveryPipeline:
    """Test 5/5: Gap detection ‚Üí Intelligent backfill ‚Üí Data recovery"""

    def test_backfill_recovery_system(self, api_client: httpx.Client):
        """
        Flujo: Detectar gaps ‚Üí Estrategia backfill ‚Üí Recuperaci√≥n datos

        Steps:
        1. Verificar estado de gaps actuales
        2. Ejecutar detecci√≥n de gaps
        3. Validar estrategia de backfill (48h rule)
        4. Confirmar que sistema se auto-recupera

        Note: Este test NO crea gaps artificiales (requiere permisos InfluxDB)
              Solo valida que el sistema de backfill est√° funcional
        """

        # Step 1: Obtener resumen de gaps
        response = api_client.get("/gaps/summary")
        assert response.status_code == 200, f"Gap summary failed: {response.text}"

        summary = response.json()

        # Step 2: Validar estructura de respuesta
        assert "ree" in summary, "Missing REE gap info"
        assert "weather" in summary, "Missing weather gap info"

        ree_gaps = summary["ree"]
        weather_gaps = summary["weather"]

        print(f"‚úÖ Backfill System: Operational")
        print(f"üìä REE gaps: {ree_gaps.get('total_gap_hours', 0)} hours")
        print(f"üìä Weather gaps: {weather_gaps.get('total_gap_hours', 0)} hours")

        # Step 3: Si hay gaps peque√±os, validar estrategia
        total_gaps = (ree_gaps.get('total_gap_hours', 0) +
                     weather_gaps.get('total_gap_hours', 0))

        if total_gaps > 0:
            print(f"‚ö†Ô∏è  Detected {total_gaps} hours of gaps")

            # Verificar que el sistema tiene mecanismo de auto-recovery
            # (APScheduler job cada 2 horas)
            print(f"‚úÖ Auto-recovery system will handle gaps within 2 hours")
        else:
            print(f"‚úÖ No gaps detected - system is healthy")

        # Step 4: Validar endpoint de detecci√≥n detallada
        response_detailed = api_client.get("/gaps/detect?days_back=7")
        assert response_detailed.status_code == 200, \
            f"Detailed gap detection failed: {response_detailed.text}"

        detailed = response_detailed.json()

        # Sistema debe devolver an√°lisis incluso si no hay gaps
        assert "analysis" in detailed or "gaps" in detailed, \
            "Gap detection should return analysis"

        print(f"‚úÖ Gap detection pipeline: Functional")


# ============================================================================
# INTEGRATION HELPERS
# ============================================================================

def wait_for_data_ingestion(client: httpx.Client, max_wait: int = 60) -> bool:
    """
    Esperar a que haya datos recientes (√∫til para tests que requieren ingesta)

    Returns:
        True si hay datos recientes, False si timeout
    """
    start = time.time()

    while time.time() - start < max_wait:
        try:
            response = client.get("/dashboard/summary")
            if response.status_code == 200:
                data = response.json()
                # Si tiene datos actuales, consideramos que la ingesta funciona
                if data.get("current_data") or data.get("status") == "healthy":
                    return True
        except Exception:
            pass

        time.sleep(5)

    return False


def validate_data_freshness(timestamp_str: str, max_age_hours: int = 1) -> bool:
    """
    Validar que un timestamp es reciente

    Args:
        timestamp_str: Timestamp ISO format
        max_age_hours: M√°ximo de horas de antig√ºedad aceptable

    Returns:
        True si los datos son frescos, False si son antiguos
    """
    try:
        timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        age = datetime.now() - timestamp.replace(tzinfo=None)
        return age.total_seconds() < (max_age_hours * 3600)
    except Exception:
        # Si no podemos parsear, asumimos que son frescos
        return True
