"""
Direct ML Training Service
Implementaci√≥n simplificada sin MLflow usando patrones probados de InfluxDB
"""

import pickle
import logging
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
import asyncio

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import r2_score, accuracy_score, classification_report, mean_absolute_error, mean_squared_error
from influxdb_client import InfluxDBClient

from services.data_ingestion import DataIngestionService
from domain.machinery.specs import (
    MACHINERY_SPECS,
    determine_active_process,
    calculate_process_energy,
    calculate_process_cost
)

logger = logging.getLogger(__name__)

class DirectMLService:
    """Servicio de ML directo sin dependencias de MLflow"""
    
    def __init__(self):
        self.models_dir = Path("/app/models")
        self.models_dir.mkdir(exist_ok=True)
        
        # Create latest directory for current models
        self.latest_dir = self.models_dir / "latest"
        self.latest_dir.mkdir(exist_ok=True)
        
        # Registry for model metadata
        self.registry_path = self.models_dir / "model_registry.json"
        
        # Initialize models as None
        self.energy_model = None
        self.production_model = None
        
        # Current model info
        self.current_timestamp = None
        
    def _generate_model_timestamp(self) -> str:
        """Generate timestamp for model versioning"""
        return datetime.now().strftime("%Y%m%d_%H%M%S")
    
    def _save_model_with_version(self, model, model_type: str, timestamp: str, metrics: Dict) -> str:
        """Save model with timestamp version and update registry"""
        
        # Create versioned filename
        versioned_filename = f"{model_type}_{timestamp}.pkl"
        versioned_path = self.models_dir / versioned_filename
        
        # Save versioned model
        with open(versioned_path, 'wb') as f:
            pickle.dump(model, f)
        
        # Create symlink in latest/ directory
        latest_path = self.latest_dir / f"{model_type}.pkl"
        if latest_path.exists() or latest_path.is_symlink():
            latest_path.unlink()
        latest_path.symlink_to(f"../{versioned_filename}")
        
        # Update registry
        self._update_model_registry(model_type, timestamp, versioned_filename, metrics)
        
        return str(versioned_path)
    
    def _update_model_registry(self, model_type: str, timestamp: str, filename: str, metrics: Dict):
        """Update model registry with new model info"""
        
        # Load existing registry or create new one
        registry = {}
        if self.registry_path.exists():
            try:
                with open(self.registry_path, 'r') as f:
                    registry = json.load(f)
            except Exception:
                registry = {}
        
        # Initialize model type if not exists
        if model_type not in registry:
            registry[model_type] = {"models": []}
        
        # Add new model entry
        new_entry = {
            "timestamp": timestamp,
            "filename": filename,
            "metrics": metrics,
            "created_at": datetime.now().isoformat(),
            "is_current": True
        }
        
        # Mark previous models as not current
        for model in registry[model_type]["models"]:
            model["is_current"] = False
        
        # Add new model
        registry[model_type]["models"].append(new_entry)
        registry[model_type]["latest"] = new_entry
        
        # Keep only last 10 versions per model type
        registry[model_type]["models"] = registry[model_type]["models"][-10:]
        
        # Save registry
        with open(self.registry_path, 'w') as f:
            json.dump(registry, f, indent=2)
        
    async def extract_siar_historical(self) -> pd.DataFrame:
        """
        Extrae TODOS los datos SIAR hist√≥ricos (88k registros, 25 a√±os).

        Returns:
            DataFrame con columnas: timestamp, temperature, humidity, day_of_year
        """
        logger.info("üìö Extrayendo SIAR hist√≥ricos (88k registros, 2000-2025)...")

        async with DataIngestionService() as service:
            siar_query = '''
                from(bucket: "siar_historical")
                |> range(start: 2000-01-01T00:00:00Z)
                |> filter(fn: (r) => r._measurement == "siar_weather")
                |> filter(fn: (r) => r._field == "temperature" or r._field == "humidity")
            '''

            try:
                query_api = service.client.query_api()
                tables = query_api.query(siar_query)

                siar_data = []
                for table in tables:
                    for record in table.records:
                        field_name = record.get_field()
                        if 'temperatura' in field_name.lower():
                            field_name = 'temperature'
                        elif 'humedad' in field_name.lower():
                            field_name = 'humidity'

                        siar_data.append({
                            'timestamp': record.get_time(),
                            'field': field_name,
                            'value': record.get_value()
                        })

                if not siar_data:
                    logger.warning("‚ö†Ô∏è No SIAR data found")
                    return pd.DataFrame()

                siar_df = pd.DataFrame(siar_data)
                siar_df['timestamp'] = pd.to_datetime(siar_df['timestamp'])

                # Pivot to get temperature and humidity
                siar_pivot = siar_df.pivot_table(
                    index='timestamp',
                    columns='field',
                    values='value',
                    aggfunc='mean'
                ).reset_index()

                # Ensure column names are lowercase
                siar_pivot.columns = siar_pivot.columns.str.lower()

                # Add day_of_year for seasonal patterns
                siar_pivot['day_of_year'] = siar_pivot['timestamp'].dt.dayofyear

                logger.info(f"‚úÖ SIAR extra√≠do: {len(siar_pivot)} registros ({siar_pivot['timestamp'].min()} ‚Üí {siar_pivot['timestamp'].max()})")
                logger.info(f"üìã Columnas disponibles: {list(siar_pivot.columns)}")
                return siar_pivot

            except Exception as e:
                logger.error(f"‚ùå Error extrayendo SIAR: {e}")
                return pd.DataFrame()

    async def extract_ree_recent(self, days_back: int = 100) -> pd.DataFrame:
        """
        Extrae datos REE recientes (√∫ltimos N d√≠as) para fine-tuning.

        Args:
            days_back: D√≠as atr√°s a extraer (default: 100)

        Returns:
            DataFrame con columnas: timestamp, price_eur_kwh
        """
        logger.info(f"‚ö° Extrayendo REE reciente ({days_back} d√≠as)...")

        async with DataIngestionService() as service:
            energy_query = f'''
                from(bucket: "{service.config.bucket}")
                |> range(start: -{days_back}d)
                |> filter(fn: (r) => r._measurement == "energy_prices")
                |> filter(fn: (r) => r._field == "price_eur_kwh")
                |> sort(columns: ["_time"])
            '''

            try:
                query_api = service.client.query_api()
                tables = query_api.query(energy_query)

                energy_data = []
                for table in tables:
                    for record in table.records:
                        energy_data.append({
                            'timestamp': record.get_time(),
                            'price_eur_kwh': record.get_value()
                        })

                if not energy_data:
                    logger.warning("‚ö†Ô∏è No REE data found")
                    return pd.DataFrame()

                ree_df = pd.DataFrame(energy_data)
                ree_df['timestamp'] = pd.to_datetime(ree_df['timestamp'])
                ree_df = ree_df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')

                logger.info(f"‚úÖ REE extra√≠do: {len(ree_df)} registros ({ree_df['timestamp'].min()} ‚Üí {ree_df['timestamp'].max()})")
                return ree_df

            except Exception as e:
                logger.error(f"‚ùå Error extrayendo REE: {e}")
                return pd.DataFrame()

    async def extract_data_from_influxdb(self, hours_back: int = 72, use_all_data: bool = False) -> pd.DataFrame:
        """
        Extrae datos usando rangos temporales expandidos + SIAR hist√≥ricos para acceder datos m√°ximos

        Si use_all_data=True:
        - Extrae TODOS los datos de energy_prices (REE 2022-2025)
        - Extrae SIAR hist√≥ricos (25 a√±os, 2000-2025)
        - Combina ambas fuentes para m√°ximo coverage
        """
        async with DataIngestionService() as service:
            # Usar rango expandido para training o rango limitado para testing
            if use_all_data:
                time_range = "start: 0"  # Todos los datos disponibles
                energy_limit = 50000  # REE: 42k+ registros disponibles
                weather_limit = 100000  # SIAR + Weather: 88k+ registros
                logger.info("üî• MAXIMIZED mode: Usando TODOS los datos hist√≥ricos (REE + SIAR)")
            else:
                time_range = f"start: -{hours_back}h"
                energy_limit = 200
                weather_limit = 400

            # Query REE data - RANGO EXPANDIDO para acceder datos hist√≥ricos
            energy_query = f'''
                from(bucket: "{service.config.bucket}")
                |> range({time_range})
                |> filter(fn: (r) => r._measurement == "energy_prices")
                |> filter(fn: (r) => r._field == "price_eur_kwh")
                |> sort(columns: ["_time"], desc: true)
                |> limit(n: {energy_limit})
            '''

            # Query Weather data - RANGO EXPANDIDO para acceder datos hist√≥ricos
            weather_query = f'''
                from(bucket: "{service.config.bucket}")
                |> range({time_range})
                |> filter(fn: (r) => r._measurement == "weather_data")
                |> filter(fn: (r) => r._field == "temperature" or r._field == "humidity")
                |> sort(columns: ["_time"], desc: true)
                |> limit(n: {weather_limit})
            '''

            # Query SIAR historical data (25 a√±os si use_all_data)
            siar_query = None
            if use_all_data:
                siar_query = f'''
                    from(bucket: "siar_historical")
                    |> range(start: 2000-01-01T00:00:00Z)
                    |> filter(fn: (r) => r._measurement == "siar_weather")
                    |> filter(fn: (r) => r._field == "temperatura_media" or r._field == "humedad_relativa_media")
                    |> limit(n: 100000)
                '''
            
            try:
                # Execute queries usando el mismo cliente que funciona
                query_api = service.client.query_api()
                energy_tables = query_api.query(energy_query)
                weather_tables = query_api.query(weather_query)

                # Process energy data
                energy_data = []
                for table in energy_tables:
                    for record in table.records:
                        energy_data.append({
                            'timestamp': record.get_time(),
                            'price_eur_kwh': record.get_value()
                        })

                # Process weather data (current + SIAR historical)
                weather_data = []
                for table in weather_tables:
                    for record in table.records:
                        weather_data.append({
                            'timestamp': record.get_time(),
                            'field': record.get_field(),
                            'value': record.get_value()
                        })

                # Process SIAR historical data if available
                if siar_query:
                    try:
                        siar_tables = query_api.query(siar_query)
                        for table in siar_tables:
                            for record in table.records:
                                # Map SIAR field names to standard names
                                field_name = record.get_field()
                                if 'temperatura' in field_name.lower():
                                    field_name = 'temperature'
                                elif 'humedad' in field_name.lower():
                                    field_name = 'humidity'

                                weather_data.append({
                                    'timestamp': record.get_time(),
                                    'field': field_name,
                                    'value': record.get_value()
                                })
                        logger.info(f"‚úÖ SIAR historical data integrated ({len(siar_tables)} tables)")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è SIAR query failed (non-critical): {e}")

                logger.info(f"Extracted {len(energy_data)} energy records, {len(weather_data)} weather records")
                
                # Convert to DataFrames
                energy_df = pd.DataFrame(energy_data)
                weather_df = pd.DataFrame(weather_data)
                
                if energy_df.empty or weather_df.empty:
                    logger.error("No data extracted from InfluxDB")
                    return pd.DataFrame()
                
                # OPCI√ìN 2: RESAMPLE weather to hourly granularity
                # Convert weather timestamp to datetime
                weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp'])

                # Extract hour and round to hourly level
                weather_df['hour'] = weather_df['timestamp'].dt.floor('H')

                # Aggregate weather by hour (mean, max, min)
                weather_hourly = weather_df.groupby('hour').agg({
                    'field': 'first',  # Keep field name
                    'value': ['mean', 'max', 'min']
                }).reset_index()

                # Flatten and rename columns for better readability
                weather_hourly.columns = ['_'.join(col).strip('_') if col[1] else col[0]
                                         for col in weather_hourly.columns.values]

                # Pivot to get temperature and humidity stats
                weather_pivoted = []
                for field in ['temperature', 'humidity']:
                    field_data = weather_df[weather_df['field'] == field].copy()
                    if not field_data.empty:
                        field_hourly = field_data.groupby('hour').agg({
                            'value': ['mean', 'max', 'min']
                        }).reset_index()
                        field_hourly.columns = ['timestamp', f'{field}_mean', f'{field}_max', f'{field}_min']
                        weather_pivoted.append(field_hourly)

                # Merge all weather fields
                if weather_pivoted:
                    weather_hourly = weather_pivoted[0]
                    for wf in weather_pivoted[1:]:
                        weather_hourly = pd.merge(weather_hourly, wf, on='timestamp', how='outer')
                else:
                    # Fallback to original pivot if no data
                    weather_hourly = weather_df.pivot_table(
                        index='timestamp',
                        columns='field',
                        values='value',
                        aggfunc='mean'
                    ).reset_index()
                    weather_hourly.rename(columns={'timestamp': 'timestamp'}, inplace=True)

                # Convert energy timestamp to datetime for merge
                energy_df['timestamp'] = pd.to_datetime(energy_df['timestamp'])

                # Merge datasets on timestamp (now both are at hourly granularity)
                df = pd.merge(energy_df, weather_hourly, on='timestamp', how='inner')

                # Fill any remaining NaN with forward/backward fill
                for col in df.columns:
                    if df[col].dtype in ['float64', 'float32']:
                        df[col] = df[col].fillna(method='ffill').fillna(method='bfill')

                # Rename weather columns to match expected names
                if 'temperature_mean' in df.columns:
                    df['temperature'] = df['temperature_mean']
                if 'humidity_mean' in df.columns:
                    df['humidity'] = df['humidity_mean']

                logger.info(f"Final merged dataset: {len(df)} records (resampled from {len(weather_df)} weather records)")
                return df
                
            except Exception as e:
                logger.error(f"Error extracting data: {e}")
                return pd.DataFrame()
    
    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Feature engineering simplificado
        """
        if df.empty:
            return df
            
        try:
            # Basic features
            df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
            df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek

            # Machine-specific features based on REAL machinery specifications
            df['active_process'] = df['hour'].apply(determine_active_process)
            df['machine_power_kw'] = df['active_process'].apply(
                lambda p: MACHINERY_SPECS[p]['power_kw']
            )
            df['machine_duration_hours'] = df['active_process'].apply(
                lambda p: MACHINERY_SPECS[p]['duration_minutes'] / 60
            )
            df['machine_optimal_temp'] = df['active_process'].apply(
                lambda p: np.mean(MACHINERY_SPECS[p]['optimal_temp_range'])
            )
            df['machine_optimal_humidity'] = df['active_process'].apply(
                lambda p: MACHINERY_SPECS[p]['optimal_humidity']
            )

            # Calculate deviations and efficiencies
            df['temp_machine_deviation'] = np.abs(
                df.get('temperature', 22) - df['machine_optimal_temp']
            )
            df['humidity_machine_deviation'] = np.abs(
                df.get('humidity', 55) - df['machine_optimal_humidity']
            )
            df['machine_thermal_efficiency'] = np.maximum(
                0, 100 - df['temp_machine_deviation'] * 5
            )
            df['machine_humidity_efficiency'] = np.maximum(
                0, 100 - df['humidity_machine_deviation'] * 2
            )

            # Real estimated energy and cost
            df['estimated_energy_kwh'] = (
                df['machine_power_kw'] * df['machine_duration_hours']
            )
            df['estimated_cost_eur'] = (
                df['estimated_energy_kwh'] * df['price_eur_kwh']
            )

            # Tariff adjustments
            df['tariff_period'] = df['hour'].apply(lambda h:
                'P1_Punta' if h in [10, 11, 12, 18, 19, 20] else
                'P2_Llano' if h in [8, 9, 14, 15, 16, 17, 22, 23] else
                'P3_Valle'
            )
            tariff_multipliers = {
                'P1_Punta': 1.3,   # +30% penalty
                'P2_Llano': 1.0,   # neutral
                'P3_Valle': 0.8    # -20% bonus
            }
            df['tariff_multiplier'] = df['tariff_period'].map(tariff_multipliers)
            df['cost_with_tariff'] = df['estimated_cost_eur'] * df['tariff_multiplier']

            # Price normalization for scoring
            df['price_normalized'] = (df['price_eur_kwh'] / 0.40) * 100

            # Energy optimization score (target for regression) - PHYSICS-BASED WITH MACHINERY SPECS
            # Multi-component scoring based on real machine thermal efficiency
            df['energy_optimization_score'] = (
                (100 - df['price_normalized']) * 0.4 +                    # 40% price weight
                df['machine_thermal_efficiency'] * 0.35 +                 # 35% thermal efficiency
                df['machine_humidity_efficiency'] * 0.15 +                # 15% humidity efficiency
                ((df['tariff_multiplier'] - 1) * -50 + 50) * 0.1         # 10% tariff weight
            ).clip(0, 100)
            
            # Production recommendation (target for classification) - PHYSICS-BASED WITH MACHINERY SPECS
            # Calculate production suitability score based on real machinery thermal/humidity efficiency
            df['production_suitability'] = (
                df['machine_thermal_efficiency'] * 0.45 +       # 45% thermal efficiency
                df['machine_humidity_efficiency'] * 0.25 +      # 25% humidity efficiency
                (100 - df['price_normalized']) * 0.30           # 30% price factor
            )

            # Adjust for extreme tariff periods
            df['production_suitability'] = df.apply(
                lambda row: row['production_suitability'] * row['tariff_multiplier']
                if row['tariff_period'] == 'P3_Valle' else row['production_suitability'],
                axis=1
            )

            # Classification based on production suitability thresholds
            conditions = [
                df['production_suitability'] >= 75,  # Optimal (high efficiency + low cost)
                (df['production_suitability'] >= 55) & (df['production_suitability'] < 75),  # Moderate
                (df['production_suitability'] >= 35) & (df['production_suitability'] < 55),  # Reduced
            ]
            choices = ['Optimal', 'Moderate', 'Reduced']
            df['production_class'] = np.select(conditions, choices, default='Halt')
            
            logger.info(f"Features engineered for {len(df)} records")
            return df
            
        except Exception as e:
            logger.error(f"Error in feature engineering: {e}")
            return df
    
    async def train_models_hybrid(self) -> Dict[str, Any]:
        """
        REAL ML TRAINING WITH ACTUAL DATA
        ==================================
        Train production state classifier using real REE + weather data.

        Approach:
        - Use real REE prices (2022-2025, 42k+ records)
        - Use real weather (SIAR 2000-2025, 88k+ records)
        - Generate target from business rules (not synthetic formulas)
        - Proper train/test split (80/20) to validate real performance

        Target: Predict production_state {Optimal, Moderate, Reduced, Halt}
        Based on: price_eur_kwh, temperature, humidity, hour, day_of_week

        Expected performance: R¬≤ 0.60-0.75 (realistic, not inflated)
        """
        logger.info("üî¨ REAL ML TRAINING WITH ACTUAL DATA")
        logger.info("üìä Using REE (42k records) + SIAR weather (88k records)")
        logger.info("‚úîÔ∏è  Proper train/test split for honest validation")

        results = {}

        try:
            timestamp = self._generate_model_timestamp()
            self.current_timestamp = timestamp

            # STEP 1: Extract real REE data (2022-2025, hourly)
            logger.info("\n" + "="*60)
            logger.info("STEP 1: Extract real REE prices (42k+ records, 2022-2025)")
            logger.info("="*60)

            ree_data = await self.extract_ree_recent(days_back=1095)  # ~3 years

            if ree_data.empty:
                logger.error("‚ùå No REE data available")
                return {"success": False, "error": "No REE data available"}

            logger.info(f"‚úÖ REE extracted: {len(ree_data)} records ({ree_data['timestamp'].min()} ‚Üí {ree_data['timestamp'].max()})")

            # STEP 2: Extract real SIAR weather data (2000-2025)
            logger.info("\n" + "="*60)
            logger.info("STEP 2: Extract real SIAR weather (88k+ records, 2000-2025)")
            logger.info("="*60)

            siar_df = await self.extract_siar_historical()

            if siar_df.empty:
                logger.error("‚ùå No SIAR data available")
                return {"success": False, "error": "No SIAR data available"}

            logger.info(f"‚úÖ SIAR extracted: {len(siar_df)} records ({siar_df['timestamp'].min()} ‚Üí {siar_df['timestamp'].max()})")

            # STEP 3: Merge REE + SIAR on matching dates/hours
            logger.info("\n" + "="*60)
            logger.info("STEP 3: Merge REE (hourly) + SIAR (daily) on date match")
            logger.info("="*60)

            # Resample REE to daily (daily average price)
            ree_data['date'] = ree_data['timestamp'].dt.date
            ree_daily = ree_data.groupby('date').agg({
                'price_eur_kwh': 'mean'
            }).reset_index()
            ree_daily['timestamp'] = pd.to_datetime(ree_daily['date'])
            ree_daily['hour'] = 12
            ree_daily['day_of_week'] = ree_daily['timestamp'].dt.dayofweek

            # Resample SIAR to daily
            siar_df['date'] = siar_df['timestamp'].dt.date
            siar_daily = siar_df.groupby('date').agg({
                'temperature': 'mean',
                'humidity': 'mean'
            }).reset_index()
            siar_daily['timestamp'] = pd.to_datetime(siar_daily['date'])

            # Merge on date
            merged_df = pd.merge(ree_daily, siar_daily, on='date', how='inner', suffixes=('_ree', '_siar'))

            logger.info(f"‚úÖ Merged data: {len(merged_df)} daily records")

            if len(merged_df) < 100:
                logger.error(f"‚ùå Insufficient merged data: {len(merged_df)} samples (need >100)")
                return {"success": False, "error": "Insufficient data after merge"}

            # STEP 4: Engineer machinery features (includes physics-based targets)
            logger.info("\n" + "="*60)
            logger.info("STEP 4: Engineer machinery features from specs (includes targets)")
            logger.info("="*60)

            merged_df = self.engineer_features(merged_df)
            logger.info(f"‚úÖ Machinery features + targets engineered: {len(merged_df)} records")
            logger.info(f"Target distribution (production_class):")
            logger.info(merged_df['production_class'].value_counts().to_string())

            # STEP 5: Prepare features for training
            logger.info("\n" + "="*60)
            logger.info("STEP 5: Prepare features for classification")
            logger.info("="*60)

            feature_columns = [
                'price_eur_kwh', 'hour', 'day_of_week', 'temperature', 'humidity',
                'machine_power_kw', 'machine_thermal_efficiency', 'machine_humidity_efficiency',
                'estimated_cost_eur', 'tariff_multiplier'
            ]

            X = merged_df[feature_columns].copy()
            X = X.fillna(X.mean())
            y = merged_df['production_class'].copy()

            logger.info(f"‚úÖ Features shape: {X.shape}")
            logger.info(f"‚úÖ Target shape: {y.shape}")
            logger.info(f"üìã Features: {feature_columns}")

            # STEP 6: Train/Test split (80/20)
            logger.info("\n" + "="*60)
            logger.info("STEP 6: Train/Test split (80/20) with proper validation")
            logger.info("="*60)

            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            logger.info(f"‚úÖ Training set: {len(X_train)} samples")
            logger.info(f"‚úÖ Test set: {len(X_test)} samples")
            logger.info(f"Training distribution: {y_train.value_counts().to_dict()}")

            # STEP 7: Train RandomForest classifier
            logger.info("\n" + "="*60)
            logger.info("STEP 7: Train RandomForest classifier")
            logger.info("="*60)

            self.production_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                class_weight='balanced'
            )
            self.production_model.fit(X_train, y_train)

            # STEP 8: Evaluate on test set
            logger.info("\n" + "="*60)
            logger.info("STEP 8: Evaluate on test set (HONEST METRICS)")
            logger.info("="*60)

            y_pred = self.production_model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)

            logger.info(f"‚úÖ Accuracy: {accuracy:.4f}")
            logger.info(f"Classification Report:")
            logger.info(classification_report(y_test, y_pred))

            # STEP 9: Train energy regression model (0-100 score)
            logger.info("\n" + "="*60)
            logger.info("STEP 9: Train energy optimization score (regression)")
            logger.info("="*60)

            # Energy score: 0-100 based on conditions
            merged_df['energy_score'] = 50  # Default
            merged_df.loc[optimal, 'energy_score'] = 95
            merged_df.loc[reduced, 'energy_score'] = 30
            merged_df.loc[halt, 'energy_score'] = 10

            y_energy = merged_df['energy_score'].copy()

            X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(
                X, y_energy, test_size=0.2, random_state=42
            )

            self.energy_model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            self.energy_model.fit(X_train_e, y_train_e)

            y_pred_e = self.energy_model.predict(X_test_e)
            r2_energy = r2_score(y_test_e, y_pred_e)

            logger.info(f"‚úÖ Energy model R¬≤: {r2_energy:.4f}")

            # STEP 10: Save models
            logger.info("\n" + "="*60)
            logger.info("STEP 10: Save models and update registry")
            logger.info("="*60)

            # Save energy model
            energy_metrics = {
                'r2_score': float(r2_energy),
                'training_samples': len(X_train_e),
                'test_samples': len(X_test_e),
                'data_source': 'REE_2022_2025 + SIAR_2000_2025',
                'samples_merged': len(merged_df),
                'strategy': 'REAL_DATA_WITH_BUSINESS_RULES'
            }

            energy_path = self._save_model_with_version(
                self.energy_model, 'energy_optimization', timestamp, energy_metrics
            )

            # Save production model
            production_metrics = {
                'accuracy': float(accuracy),
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'data_source': 'REE_2022_2025 + SIAR_2000_2025',
                'samples_merged': len(merged_df),
                'strategy': 'REAL_DATA_WITH_BUSINESS_RULES',
                'classes': ['Optimal', 'Moderate', 'Reduced', 'Halt']
            }

            production_path = self._save_model_with_version(
                self.production_model, 'production_classifier', timestamp, production_metrics
            )

            logger.info(f"‚úÖ Models saved:")
            logger.info(f"  - Energy model: {energy_path}")
            logger.info(f"  - Production model: {production_path}")

            # Build final results
            results['success'] = True
            results['energy_model'] = {
                **energy_metrics,
                'saved': True,
                'model_path': energy_path,
                'timestamp': timestamp,
                'strategy': 'REAL_DATA_WITH_BUSINESS_RULES'
            }
            results['production_model'] = {
                **production_metrics,
                'saved': True,
                'model_path': production_path,
                'timestamp': timestamp,
                'strategy': 'REAL_DATA_WITH_BUSINESS_RULES'
            }

            logger.info(f"\n‚úÖ REAL ML TRAINING COMPLETE")
            logger.info(f"üìä Merged data samples: {len(merged_df)}")
            logger.info(f"üìä Training set: {len(X_train)} samples")
            logger.info(f"üìä Test set: {len(X_test)} samples")
            logger.info(f"üìà Energy R¬≤ Score: {r2_energy:.4f}")
            logger.info(f"üìà Production Accuracy: {accuracy:.4f}")
            logger.info(f"üíæ Data source: REE (2022-2025, 42k records) + SIAR (2000-2025, 88k records)")
            logger.info(f"‚úîÔ∏è  Business rules-based target (not synthetic)")
            logger.info(f"‚úîÔ∏è  Proper train/test split for honest validation")

            results['total_samples'] = len(merged_df)
            results['features_used'] = feature_columns
            results['timestamp'] = datetime.now().isoformat()
            results['training_mode'] = 'REAL_DATA_BUSINESS_RULES'

            return results

        except Exception as e:
            logger.error(f"‚ùå Error in training: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    async def train_models(self) -> Dict[str, Any]:
        """
        Entrena ambos modelos directamente
        """
        logger.info("Starting direct ML training...")

        # Extract data using expanded range to access historical data
        df = await self.extract_data_from_influxdb(use_all_data=True)  # All available data

        if df.empty:
            logger.error("No data available for training")
            return {"success": False, "error": "No data available"}

        # Engineer features
        df = self.engineer_features(df)
        
        if df.empty:
            logger.error("Feature engineering failed")
            return {"success": False, "error": "Feature engineering failed"}
        
        # Prepare features - EXPANDED with machinery specs (10 features)
        feature_columns = [
            'price_eur_kwh', 'hour', 'day_of_week', 'temperature', 'humidity',
            'machine_power_kw', 'machine_thermal_efficiency', 'machine_humidity_efficiency',
            'estimated_cost_eur', 'tariff_multiplier'
        ]

        # Clean data: remove rows with NaN in critical columns and fill remaining
        df_clean = df.dropna(subset=['price_eur_kwh']).copy()

        # Ensure all required columns exist with safe defaults
        # hour and day_of_week are created by engineer_features(), so they should exist
        # temperature and humidity may be missing from some data sources
        if 'temperature' not in df_clean.columns:
            df_clean['temperature'] = 20.0  # Default comfortable temperature
        if 'humidity' not in df_clean.columns:
            df_clean['humidity'] = 50.0  # Default comfortable humidity

        X = df_clean[feature_columns].fillna(df_clean[feature_columns].mean())
        
        results = {}
        
        try:
            # Generate timestamp for this training session
            timestamp = self._generate_model_timestamp()
            self.current_timestamp = timestamp
            
            # Train Energy Optimization Model (Regression)
            y_energy = df_clean['energy_optimization_score'].fillna(df_clean['energy_optimization_score'].mean())
            
            if len(X) >= 10:  # Minimum samples needed
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y_energy, test_size=0.2, random_state=42
                )
                
                self.energy_model = RandomForestRegressor(
                    n_estimators=100,
                    max_depth=15,
                    min_samples_split=5,
                    random_state=42
                )
                self.energy_model.fit(X_train, y_train)

                # Evaluate on TRAIN (overfitting detection)
                y_pred_train = self.energy_model.predict(X_train)
                r2_train = r2_score(y_train, y_pred_train)
                mae_train = mean_absolute_error(y_train, y_pred_train)
                rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))

                # Evaluate on TEST
                y_pred_test = self.energy_model.predict(X_test)
                r2_test = r2_score(y_test, y_pred_test)
                mae_test = mean_absolute_error(y_test, y_pred_test)
                rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

                # Cross-validation (5-fold)
                logger.info("üîç Running 5-fold cross-validation...")
                kfold = KFold(n_splits=5, shuffle=True, random_state=42)
                cv_scores = cross_val_score(
                    self.energy_model, X, y_energy, cv=kfold,
                    scoring='r2', n_jobs=-1
                )
                cv_r2_mean = cv_scores.mean()
                cv_r2_std = cv_scores.std()

                # Overfitting detection
                r2_diff = r2_train - r2_test
                overfitting_detected = r2_diff > 0.10  # Threshold 10%

                logger.info("="*60)
                logger.info("ENERGY MODEL VALIDATION METRICS")
                logger.info("="*60)
                logger.info(f"  R¬≤ Train:     {r2_train:.4f}")
                logger.info(f"  R¬≤ Test:      {r2_test:.4f}")
                logger.info(f"  R¬≤ Diff:      {r2_diff:.4f} {'‚ö†Ô∏è  OVERFITTING' if overfitting_detected else '‚úÖ OK'}")
                logger.info(f"  MAE Train:    {mae_train:.4f}")
                logger.info(f"  MAE Test:     {mae_test:.4f}")
                logger.info(f"  RMSE Train:   {rmse_train:.4f}")
                logger.info(f"  RMSE Test:    {rmse_test:.4f}")
                logger.info(f"  CV R¬≤ (5-fold): {cv_r2_mean:.4f} ¬± {cv_r2_std:.4f}")
                logger.info("="*60)

                # Feature importance
                feature_importance = dict(zip(
                    feature_columns,
                    self.energy_model.feature_importances_
                ))
                sorted_importance = sorted(
                    feature_importance.items(),
                    key=lambda x: x[1],
                    reverse=True
                )
                logger.info("Top 5 feature importance:")
                for feat, imp in sorted_importance[:5]:
                    logger.info(f"  {feat}: {imp:.4f}")

                # Save model with extended metrics
                energy_metrics = {
                    'r2_train': float(r2_train),
                    'r2_test': float(r2_test),
                    'r2_diff': float(r2_diff),
                    'mae_train': float(mae_train),
                    'mae_test': float(mae_test),
                    'rmse_train': float(rmse_train),
                    'rmse_test': float(rmse_test),
                    'cv_r2_mean': float(cv_r2_mean),
                    'cv_r2_std': float(cv_r2_std),
                    'overfitting_detected': bool(overfitting_detected),
                    'training_samples': len(X_train),
                    'test_samples': len(X_test),
                    'feature_importance': {k: float(v) for k, v in sorted_importance[:5]}
                }
                energy_path = self._save_model_with_version(
                    self.energy_model, 'energy_optimization', timestamp, energy_metrics
                )

                results['energy_model'] = {
                    **energy_metrics,
                    'saved': True,
                    'model_path': energy_path,
                    'timestamp': timestamp
                }

                logger.info(f"‚úÖ Energy model trained and validated (saved as {timestamp})")
            
            # Train Production Classification Model
            # Filter data to match production class indices
            production_data = df_clean.dropna(subset=['production_class'])
            if len(production_data) >= 10:
                X_prod = production_data[feature_columns].fillna(production_data[feature_columns].mean())
                y_production = production_data['production_class']

                if len(y_production.unique()) > 1:
                    # Check if all classes have at least 2 samples for stratification
                    class_counts = y_production.value_counts()
                    can_stratify = all(count >= 2 for count in class_counts.values)

                    X_train, X_test, y_train, y_test = train_test_split(
                        X_prod, y_production, test_size=0.2, random_state=42,
                        stratify=y_production if can_stratify else None
                    )
                    
                    self.production_model = RandomForestClassifier(
                        n_estimators=100,
                        max_depth=15,
                        min_samples_split=5,
                        class_weight='balanced',
                        random_state=42
                    )
                    self.production_model.fit(X_train, y_train)

                    # Evaluate on TRAIN (overfitting detection)
                    y_pred_train = self.production_model.predict(X_train)
                    accuracy_train = accuracy_score(y_train, y_pred_train)

                    # Evaluate on TEST
                    y_pred_test = self.production_model.predict(X_test)
                    accuracy_test = accuracy_score(y_test, y_pred_test)

                    # Cross-validation (5-fold)
                    logger.info("üîç Running 5-fold cross-validation...")
                    cv_scores = cross_val_score(
                        self.production_model, X_prod, y_production, cv=kfold,
                        scoring='accuracy', n_jobs=-1
                    )
                    cv_accuracy_mean = cv_scores.mean()
                    cv_accuracy_std = cv_scores.std()

                    # Overfitting detection
                    accuracy_diff = accuracy_train - accuracy_test
                    overfitting_detected = accuracy_diff > 0.15  # Threshold 15%

                    logger.info("="*60)
                    logger.info("PRODUCTION MODEL VALIDATION METRICS")
                    logger.info("="*60)
                    logger.info(f"  Accuracy Train: {accuracy_train:.4f}")
                    logger.info(f"  Accuracy Test:  {accuracy_test:.4f}")
                    logger.info(f"  Accuracy Diff:  {accuracy_diff:.4f} {'‚ö†Ô∏è  OVERFITTING' if overfitting_detected else '‚úÖ OK'}")
                    logger.info(f"  CV Accuracy (5-fold): {cv_accuracy_mean:.4f} ¬± {cv_accuracy_std:.4f}")
                    logger.info("="*60)

                    # Classification report
                    logger.info("Classification Report (Test Set):")
                    logger.info("\n" + classification_report(y_test, y_pred_test))

                    # Feature importance
                    feature_importance = dict(zip(
                        feature_columns,
                        self.production_model.feature_importances_
                    ))
                    sorted_importance = sorted(
                        feature_importance.items(),
                        key=lambda x: x[1],
                        reverse=True
                    )
                    logger.info("Top 5 feature importance:")
                    for feat, imp in sorted_importance[:5]:
                        logger.info(f"  {feat}: {imp:.4f}")

                    # Save model with extended metrics
                    production_metrics = {
                        'accuracy_train': float(accuracy_train),
                        'accuracy_test': float(accuracy_test),
                        'accuracy_diff': float(accuracy_diff),
                        'cv_accuracy_mean': float(cv_accuracy_mean),
                        'cv_accuracy_std': float(cv_accuracy_std),
                        'overfitting_detected': bool(overfitting_detected),
                        'training_samples': len(X_train),
                        'test_samples': len(X_test),
                        'classes': list(y_production.unique()),
                        'feature_importance': {k: float(v) for k, v in sorted_importance[:5]}
                    }
                    production_path = self._save_model_with_version(
                        self.production_model, 'production_classifier', timestamp, production_metrics
                    )

                    results['production_model'] = {
                        **production_metrics,
                        'saved': True,
                        'model_path': production_path,
                        'timestamp': timestamp
                    }

                    logger.info(f"‚úÖ Production model trained and validated (saved as {timestamp})")
            
            results['success'] = True
            results['total_samples'] = len(df)
            results['features_used'] = feature_columns
            results['timestamp'] = datetime.now().isoformat()
            
            return results
            
        except Exception as e:
            logger.error(f"Error training models: {e}")
            return {"success": False, "error": str(e)}
    
    def load_models(self) -> bool:
        """
        Carga modelos m√°s recientes desde disco (versionado)
        """
        try:
            # Try to load from latest/ directory (symlinks to current models)
            energy_latest = self.latest_dir / "energy_optimization.pkl"
            production_latest = self.latest_dir / "production_classifier.pkl"
            
            if energy_latest.exists():
                with open(energy_latest, 'rb') as f:
                    self.energy_model = pickle.load(f)
                logger.info("Energy model loaded (latest version)")
            
            if production_latest.exists():
                with open(production_latest, 'rb') as f:
                    self.production_model = pickle.load(f)
                logger.info("Production model loaded (latest version)")
            
            return True
            
        except Exception as e:
            logger.error(f"Error loading models: {e}")
            return False
    
    def get_model_registry(self) -> Dict:
        """
        Get model registry with version history
        """
        try:
            if self.registry_path.exists():
                with open(self.registry_path, 'r') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logger.error(f"Error loading model registry: {e}")
            return {}
    
    def predict_energy_optimization(self, price_eur_kwh: float, temperature: float = 20, humidity: float = 50) -> Dict[str, Any]:
        """
        Predicci√≥n de optimizaci√≥n energ√©tica
        """
        if not self.energy_model:
            self.load_models()

        if not self.energy_model:
            return {"error": "Energy model not available"}

        try:
            # Calculate machinery features based on current hour
            now = datetime.now()
            active_process = determine_active_process(now.hour)
            machine_specs = MACHINERY_SPECS[active_process]

            machine_power_kw = machine_specs['power_kw']
            machine_duration_hours = machine_specs['duration_minutes'] / 60
            machine_optimal_temp = np.mean(machine_specs['optimal_temp_range'])
            machine_optimal_humidity = machine_specs['optimal_humidity']

            # Calculate efficiencies
            temp_deviation = abs(temperature - machine_optimal_temp)
            humidity_deviation = abs(humidity - machine_optimal_humidity)
            machine_thermal_efficiency = max(0, 100 - temp_deviation * 5)
            machine_humidity_efficiency = max(0, 100 - humidity_deviation * 2)

            # Calculate cost
            estimated_energy_kwh = machine_power_kw * machine_duration_hours
            estimated_cost_eur = estimated_energy_kwh * price_eur_kwh

            # Tariff multiplier
            if now.hour in [10, 11, 12, 18, 19, 20]:
                tariff_multiplier = 1.3  # P1_Punta
            elif now.hour in [8, 9, 14, 15, 16, 17, 22, 23]:
                tariff_multiplier = 1.0  # P2_Llano
            else:
                tariff_multiplier = 0.8  # P3_Valle

            # Prepare features - match training features (10 features with machinery specs)
            features = np.array([[
                price_eur_kwh,
                now.hour,
                now.weekday(),
                temperature,
                humidity,
                machine_power_kw,
                machine_thermal_efficiency,
                machine_humidity_efficiency,
                estimated_cost_eur,
                tariff_multiplier
            ]])

            # Get prediction from model
            prediction = self.energy_model.predict(features)[0]

            return {
                "energy_optimization_score": round(prediction, 2),
                "recommendation": "Optimal" if prediction > 70 else "Moderate" if prediction > 40 else "Reduced",
                "timestamp": now.isoformat(),
                "active_process": active_process,
                "machine_thermal_efficiency": round(machine_thermal_efficiency, 2)
            }
            
        except Exception as e:
            logger.error(f"Error in energy prediction: {e}")
            return {"error": str(e)}
    
    def predict_production_recommendation(self, price_eur_kwh: float, temperature: float = 20, humidity: float = 50) -> Dict[str, Any]:
        """
        Predicci√≥n de recomendaci√≥n de producci√≥n
        """
        if not self.production_model:
            self.load_models()

        if not self.production_model:
            return {"error": "Production model not available"}

        try:
            # Calculate machinery features based on current hour
            now = datetime.now()
            active_process = determine_active_process(now.hour)
            machine_specs = MACHINERY_SPECS[active_process]

            machine_power_kw = machine_specs['power_kw']
            machine_duration_hours = machine_specs['duration_minutes'] / 60
            machine_optimal_temp = np.mean(machine_specs['optimal_temp_range'])
            machine_optimal_humidity = machine_specs['optimal_humidity']

            # Calculate efficiencies
            temp_deviation = abs(temperature - machine_optimal_temp)
            humidity_deviation = abs(humidity - machine_optimal_humidity)
            machine_thermal_efficiency = max(0, 100 - temp_deviation * 5)
            machine_humidity_efficiency = max(0, 100 - humidity_deviation * 2)

            # Calculate cost
            estimated_energy_kwh = machine_power_kw * machine_duration_hours
            estimated_cost_eur = estimated_energy_kwh * price_eur_kwh

            # Tariff multiplier
            if now.hour in [10, 11, 12, 18, 19, 20]:
                tariff_multiplier = 1.3  # P1_Punta
            elif now.hour in [8, 9, 14, 15, 16, 17, 22, 23]:
                tariff_multiplier = 1.0  # P2_Llano
            else:
                tariff_multiplier = 0.8  # P3_Valle

            # Prepare features - match training features (10 features with machinery specs)
            features = np.array([[
                price_eur_kwh,
                now.hour,
                now.weekday(),
                temperature,
                humidity,
                machine_power_kw,
                machine_thermal_efficiency,
                machine_humidity_efficiency,
                estimated_cost_eur,
                tariff_multiplier
            ]])

            prediction = self.production_model.predict(features)[0]
            probabilities = self.production_model.predict_proba(features)[0]

            # Get class probabilities
            classes = self.production_model.classes_
            prob_dict = {classes[i]: round(prob, 3) for i, prob in enumerate(probabilities)}

            return {
                "production_recommendation": prediction,
                "confidence": round(max(probabilities), 3),
                "probabilities": prob_dict,
                "timestamp": now.isoformat(),
                "active_process": active_process,
                "machine_thermal_efficiency": round(machine_thermal_efficiency, 2)
            }
            
        except Exception as e:
            logger.error(f"Error in production prediction: {e}")
            return {"error": str(e)}
    
    def get_models_status(self) -> Dict[str, Any]:
        """
        Estado de los modelos con informaci√≥n de versionado
        """
        # Get registry info
        registry = self.get_model_registry()
        
        # Check latest models
        energy_latest = self.latest_dir / "energy_optimization.pkl"
        production_latest = self.latest_dir / "production_classifier.pkl"
        
        # Count versioned models
        versioned_models = list(self.models_dir.glob("*_*.pkl"))
        energy_versions = [f for f in versioned_models if f.name.startswith("energy_optimization_")]
        production_versions = [f for f in versioned_models if f.name.startswith("production_classifier_")]
        
        return {
            "energy_model": {
                "loaded": self.energy_model is not None,
                "latest_exists": energy_latest.exists(),
                "current_version": registry.get("energy_optimization", {}).get("latest", {}).get("timestamp", "unknown"),
                "total_versions": len(energy_versions),
                "last_modified": energy_latest.stat().st_mtime if energy_latest.exists() else None,
                "metrics": registry.get("energy_optimization", {}).get("latest", {}).get("metrics", {})
            },
            "production_model": {
                "loaded": self.production_model is not None,
                "latest_exists": production_latest.exists(),
                "current_version": registry.get("production_classifier", {}).get("latest", {}).get("timestamp", "unknown"),
                "total_versions": len(production_versions),
                "last_modified": production_latest.stat().st_mtime if production_latest.exists() else None,
                "metrics": registry.get("production_classifier", {}).get("latest", {}).get("metrics", {})
            },
            "models_directory": str(self.models_dir),
            "versioning": {
                "enabled": True,
                "registry_exists": self.registry_path.exists(),
                "total_versioned_models": len(versioned_models)
            },
            "timestamp": datetime.now().isoformat()
        }

    def get_diagnostic_report(self) -> Dict[str, Any]:
        """
        Genera reporte diagn√≥stico completo de modelos entrenados
        Incluye m√©tricas de validaci√≥n extendidas desde el registry
        """
        try:
            registry = self.get_model_registry()

            # Energy model diagnostics
            energy_diagnostics = {}
            if "energy_optimization" in registry:
                latest = registry["energy_optimization"].get("latest", {})
                metrics = latest.get("metrics", {})

                # Overfitting analysis
                r2_train = metrics.get("r2_train", 0)
                r2_test = metrics.get("r2_test", 0)
                r2_diff = metrics.get("r2_diff", 0)
                overfitting = metrics.get("overfitting_detected", False)

                # Cross-validation stability
                cv_mean = metrics.get("cv_r2_mean", 0)
                cv_std = metrics.get("cv_r2_std", 0)
                cv_stability = "high" if cv_std < 0.05 else "medium" if cv_std < 0.10 else "low"

                energy_diagnostics = {
                    "model_type": "RandomForestRegressor",
                    "status": "healthy" if not overfitting and r2_test > 0.70 else "warning" if r2_test > 0.50 else "poor",
                    "performance": {
                        "r2_train": r2_train,
                        "r2_test": r2_test,
                        "r2_diff": r2_diff,
                        "mae_test": metrics.get("mae_test", 0),
                        "rmse_test": metrics.get("rmse_test", 0)
                    },
                    "cross_validation": {
                        "cv_r2_mean": cv_mean,
                        "cv_r2_std": cv_std,
                        "stability": cv_stability,
                        "interpretation": f"Model performance varies ¬±{cv_std:.2%} across folds"
                    },
                    "overfitting_analysis": {
                        "detected": overfitting,
                        "r2_gap": r2_diff,
                        "threshold": 0.10,
                        "interpretation": "‚ö†Ô∏è OVERFITTING: Train R¬≤ >> Test R¬≤" if overfitting else "‚úÖ No significant overfitting detected"
                    },
                    "feature_importance": metrics.get("feature_importance", {}),
                    "training_info": {
                        "training_samples": metrics.get("training_samples", 0),
                        "test_samples": metrics.get("test_samples", 0),
                        "last_trained": latest.get("created_at", "unknown")
                    },
                    "recommendations": self._generate_model_recommendations(
                        overfitting, r2_test, cv_std, "regression"
                    )
                }

            # Production model diagnostics
            production_diagnostics = {}
            if "production_classifier" in registry:
                latest = registry["production_classifier"].get("latest", {})
                metrics = latest.get("metrics", {})

                # Overfitting analysis
                acc_train = metrics.get("accuracy_train", 0)
                acc_test = metrics.get("accuracy_test", 0)
                acc_diff = metrics.get("accuracy_diff", 0)
                overfitting = metrics.get("overfitting_detected", False)

                # Cross-validation stability
                cv_mean = metrics.get("cv_accuracy_mean", 0)
                cv_std = metrics.get("cv_accuracy_std", 0)
                cv_stability = "high" if cv_std < 0.05 else "medium" if cv_std < 0.10 else "low"

                production_diagnostics = {
                    "model_type": "RandomForestClassifier",
                    "status": "healthy" if not overfitting and acc_test > 0.80 else "warning" if acc_test > 0.60 else "poor",
                    "performance": {
                        "accuracy_train": acc_train,
                        "accuracy_test": acc_test,
                        "accuracy_diff": acc_diff,
                        "classes": metrics.get("classes", [])
                    },
                    "cross_validation": {
                        "cv_accuracy_mean": cv_mean,
                        "cv_accuracy_std": cv_std,
                        "stability": cv_stability,
                        "interpretation": f"Model accuracy varies ¬±{cv_std:.2%} across folds"
                    },
                    "overfitting_analysis": {
                        "detected": overfitting,
                        "accuracy_gap": acc_diff,
                        "threshold": 0.15,
                        "interpretation": "‚ö†Ô∏è OVERFITTING: Train Acc >> Test Acc" if overfitting else "‚úÖ No significant overfitting detected"
                    },
                    "feature_importance": metrics.get("feature_importance", {}),
                    "training_info": {
                        "training_samples": metrics.get("training_samples", 0),
                        "test_samples": metrics.get("test_samples", 0),
                        "last_trained": latest.get("created_at", "unknown")
                    },
                    "recommendations": self._generate_model_recommendations(
                        overfitting, acc_test, cv_std, "classification"
                    )
                }

            # Overall system health
            overall_status = "healthy"
            if energy_diagnostics.get("status") == "poor" or production_diagnostics.get("status") == "poor":
                overall_status = "poor"
            elif energy_diagnostics.get("status") == "warning" or production_diagnostics.get("status") == "warning":
                overall_status = "warning"

            return {
                "overall_status": overall_status,
                "timestamp": datetime.now().isoformat(),
                "energy_model": energy_diagnostics,
                "production_model": production_diagnostics,
                "summary": {
                    "energy_overfitting": energy_diagnostics.get("overfitting_analysis", {}).get("detected", False),
                    "production_overfitting": production_diagnostics.get("overfitting_analysis", {}).get("detected", False),
                    "energy_r2_test": energy_diagnostics.get("performance", {}).get("r2_test", 0),
                    "production_accuracy_test": production_diagnostics.get("performance", {}).get("accuracy_test", 0)
                }
            }

        except Exception as e:
            logger.error(f"Error generating diagnostic report: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}

    def _generate_model_recommendations(self, overfitting: bool, performance: float, cv_std: float, model_type: str) -> List[str]:
        """Genera recomendaciones basadas en m√©tricas de validaci√≥n"""
        recommendations = []

        if overfitting:
            recommendations.append("‚ö†Ô∏è CRITICAL: Increase regularization (max_depth, min_samples_split)")
            recommendations.append("Consider pruning or reducing model complexity")
            recommendations.append("Collect more training data to reduce overfitting")

        if model_type == "regression":
            if performance < 0.50:
                recommendations.append("‚ùå Poor R¬≤ test score - consider feature engineering")
            elif performance < 0.70:
                recommendations.append("‚ö†Ô∏è Moderate R¬≤ - model may benefit from more features or data")
        else:  # classification
            if performance < 0.60:
                recommendations.append("‚ùå Poor accuracy - verify class balance and features")
            elif performance < 0.80:
                recommendations.append("‚ö†Ô∏è Moderate accuracy - consider ensemble methods or more data")

        if cv_std > 0.10:
            recommendations.append("‚ö†Ô∏è High CV variance - model unstable across folds")
            recommendations.append("Consider stratified sampling or more balanced data")

        if not recommendations:
            recommendations.append("‚úÖ Model performing well - no critical issues detected")

        return recommendations